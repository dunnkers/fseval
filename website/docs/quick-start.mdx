---
sidebar_position: 1
---

# Quick start

<!-- Docusaurus -->
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FileTreeCodeViewer from '../src/components/FileTreeCodeViewer';
import '../src/css/file-tree.css';


<!-- YAML files -->
import ConfDatasetSyntheticYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/dataset/synthetic.yaml';
import ConfRankerAnovaYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/ranker/anova.yaml';
import ConfRankerMutualInfoYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/ranker/mutual_info.yaml';
import ConfValidatorKnnYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/validator/knn.yaml';
import ConfMyConfigYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/my_config.yaml';
import BenchmarkPyYAML from '!!raw-loader!../../examples/quick-start-yaml/benchmark.py';

<!-- Python files -->
import ConfDatasetSyntheticPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/dataset/synthetic.py';
import ConfRankerAnovaPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/ranker/anova.py';
import ConfRankerMutualInfoPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/ranker/mutual_info.py';
import ConfValidatorKnnPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/validator/knn.py';
import ConfMyConfigPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/my_config.py';
import BenchmarkPy from '!!raw-loader!../../examples/quick-start-structured-configs/benchmark.py';

<!-- asciinema -->
import AsciinemaPlayer from '../src/components/AsciinemaPlayer';
import 'asciinema-player/dist/bundle/asciinema-player.css';

fseval helps you benchmark **Feature Selection** and **Feature Ranking** algorithms. Any algorithm that ranks features in importance.

It comes useful if you are one of the following types of users:
1. **Feature Selection / Feature Ranker algorithm authors**. You are the author of a novel Feature Selection algorithm. Now, you have to prove the performance of your algorithm against other competitors. Therefore, you are going to run a large-scale benchmark. Many authors, however, spend much time rewriting similar pipelines to benchmark their algorithms. fseval helps you run benchmarks in a structured manner, on supercomputer clusters or on the cloud.
1. **Interpretable AI method authors**. You wrote a new Interpretable AI method that aims to find out which features are most meaningful by ranking them. Now, the challenge is to find out how well your method ranked those features. fseval can help with this.
1. **Machine Learning practitioners**. You have a dataset and want to find out with exactly what features your models will perform best. You can use fseval to try multiple Feature Selection or Feature Ranking algorithms.



Key features ðŸš€:
- Easily benchmark Feature Ranking algorithms
- Built on [Hydra](https://hydra.cc/)
- Support for distributed systems (SLURM through the [Submitit launcher](https://hydra.cc/docs/plugins/submitit_launcher), AWS support through the [Ray launcher](https://hydra.cc/docs/plugins/ray_launcher/))
- Reproducible experiments (your entire experiment can be described and reproduced by 1 YAML file)
- Send experiment results directly to a dashboard (integration with [Weights and Biases](https://wandb.ai/) is built-in)
- Export your data to any SQL database (integration with [SQLAlchemy](https://www.sqlalchemy.org/))

## Getting started

Install fseval:
```shell
pip install fseval
```


<Tabs groupId="config-representation">
<TabItem value="yaml" label="YAML" default>

Given the following [configuration](https://github.com/dunnkers/fseval/tree/master/examples/quick-start-yaml):

<FileTreeCodeViewer treeId="tree-1" template={{
  root: {
    "conf": {
      "dataset": {
        "synthetic.yaml": ConfDatasetSyntheticYAML
      },
      "ranker": {
        "anova.yaml": ConfRankerAnovaYAML,
        "mutual_info.yaml": ConfRankerMutualInfoYAML,
      },
      "validator": {
        "knn.yaml": ConfValidatorKnnYAML
      },
      "my_config.yaml": ConfMyConfigYAML,
    },
    "benchmark.py": BenchmarkPyYAML,
  }
}} viewState={{
  "tree-1": {
      expandedItems: ["conf", "ranker", "dataset", "validator"],
      selectedItems: ["my_config.yaml"]
  }
}} />

</TabItem>
<TabItem value="structured" label="Structured Config">

Given the following [configuration](https://github.com/dunnkers/fseval/tree/master/examples/quick-start-structured-configs):

<FileTreeCodeViewer treeId="tree-2" template={{
  root: {
    "conf": {
      "dataset": {
        "synthetic.py": ConfDatasetSyntheticPy
      },
      "ranker": {
        "anova.py": ConfRankerAnovaPy,
        "mutual_info.py": ConfRankerMutualInfoPy,
      },
      "validator": {
        "knn.py": ConfValidatorKnnPy
      },
      "my_config.py": ConfMyConfigPy,
    },
    "benchmark.py": BenchmarkPy,
  }
}} viewState={{
  "tree-2": {
      expandedItems: ["conf", "ranker", "dataset", "validator"],
      selectedItems: ["my_config.py"]
  }
}} />

</TabItem>
</Tabs>



<br/>

Let's export the results to a local SQLite database.
```shell
results_db=/Users/dunnkers/Downloads/results.sqlite # edit this url
```

We can now run the benchmark:
```shell
python benchmark.py --multirun ranker='glob(*)' +callbacks.to_sql.url=sqlite:///$results_db
```

<AsciinemaPlayer src="/fseval/cast/examples_quick_start.cast" rows={30} idleTimeLimit={3} preload={true} />
<br/>

The results are now stored in a SQLite database. We can open the data using [DB Browser for SQLite](https://sqlitebrowser.org/). We can access the validation scores in the `validation_scores` table:

![validation data](/img/quick-start/validation_data.png)

In the example above, 
the graph plots the **feature subset size** (`n_features_to_select`) vs. **classification accuracy** (`score`)
for <span style={{backgroundColor: "#5050FF", color: "white"}}>ANOVA F value</span> vs. <span style={{backgroundColor: "#0075DD", color: "white"}}>Mutual Info</span>.
This way, we can easily compare two feature selectors: ANOVA F Value and Mutual Info âœ¨.