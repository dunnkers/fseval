---
sidebar_position: 1
---

# Getting started

<!-- Docusaurus -->
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FileTreeCodeViewer from '../src/components/FileTreeCodeViewer';
import '../src/css/file-tree.css';


<!-- YAML files -->
import ConfDatasetSyntheticYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/dataset/synthetic.yaml';
import ConfRankerAnovaYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/ranker/anova.yaml';
import ConfRankerMutualInfoYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/ranker/mutual_info.yaml';
import ConfValidatorKnnYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/validator/knn.yaml';
import ConfMyConfigYAML from '!!raw-loader!../../examples/quick-start-yaml/conf/my_config.yaml';
import BenchmarkPyYAML from '!!raw-loader!../../examples/quick-start-yaml/benchmark.py';

<!-- Python files -->
import ConfDatasetSyntheticPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/dataset/synthetic.py';
import ConfRankerAnovaPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/ranker/anova.py';
import ConfRankerMutualInfoPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/ranker/mutual_info.py';
import ConfValidatorKnnPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/validator/knn.py';
import ConfMyConfigPy from '!!raw-loader!../../examples/quick-start-structured-configs/conf/my_config.py';
import BenchmarkPy from '!!raw-loader!../../examples/quick-start-structured-configs/benchmark.py';

<!-- asciinema -->
import AsciinemaPlayer from '../src/components/AsciinemaPlayer';
import 'asciinema-player/dist/bundle/asciinema-player.css';

To get started, there's two main resources.

1. A [Google Colab](https://colab.research.google.com/drive/1Bsuxxuw0-mEsYRSnNbmvD_wNUAkOPiQa?usp=sharing)
  
  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Bsuxxuw0-mEsYRSnNbmvD_wNUAkOPiQa?usp=sharing)
2. The [‚ö°Ô∏è Quick start](#%EF%B8%8F-quick-start) guide below üëáüèª


## ‚ö°Ô∏è Quick start

Let's run our first experiment. The goal will be to compare two feature selectors _ANOVA F-Value_ and _Mutual Info_.

First, install fseval:

```
pip install fseval
```


:::note Installing from source

If for some reason you would not like to install `fseval` from the PyPi package index using `pip install` like above, you can also install `fseval` right from its Git source. Execute the following:

```
git clone https://github.com/dunnkers/fseval.git
cd fseval
pip install -r requirements.txt
pip install .
```

You should now be able to continue in the same way as before ‚úì.

:::


Now you can decide whether you want to define your configuration in _YAML_ or in _Python_. Choose whatever you find most convenient.


<Tabs groupId="config-representation">
<TabItem value="yaml" label="YAML" default>

Download the example configuration: [quick-start-yaml.zip](pathname:///fseval/zipped-examples/quick-start-yaml.zip)

Then, `cd` into the example directory. You should now have the following files:

<FileTreeCodeViewer treeId="tree-1" template={{
  root: {
    "conf": {
      "dataset": {
        "synthetic.yaml": ConfDatasetSyntheticYAML
      },
      "ranker": {
        "anova.yaml": ConfRankerAnovaYAML,
        "mutual_info.yaml": ConfRankerMutualInfoYAML,
      },
      "validator": {
        "knn.yaml": ConfValidatorKnnYAML
      },
      "my_config.yaml": ConfMyConfigYAML,
    },
    "benchmark.py": BenchmarkPyYAML,
  }
}} viewState={{
  "tree-1": {
      expandedItems: ["conf", "ranker", "dataset", "validator"],
      selectedItems: ["my_config.yaml"]
  }
}} />

</TabItem>
<TabItem value="structured" label="Structured Config">

Download the example configuration: [quick-start-structured-configs.zip](pathname:///fseval/zipped-examples/quick-start-structured-configs.zip)

You should now have the following files:

<FileTreeCodeViewer treeId="tree-2" template={{
  root: {
    "conf": {
      "dataset": {
        "synthetic.py": ConfDatasetSyntheticPy
      },
      "ranker": {
        "anova.py": ConfRankerAnovaPy,
        "mutual_info.py": ConfRankerMutualInfoPy,
      },
      "validator": {
        "knn.py": ConfValidatorKnnPy
      },
      "my_config.py": ConfMyConfigPy,
    },
    "benchmark.py": BenchmarkPy,
  }
}} viewState={{
  "tree-2": {
      expandedItems: ["conf", "ranker", "dataset", "validator"],
      selectedItems: ["my_config.py"]
  }
}} />

</TabItem>
</Tabs>



<br/>

We can now decide how to export the results. We can upload our results to a live SQL database. For now, let's choose a local database. SQLite is good for this.

```shell
sql_con=sqlite:////Users/dunnkers/Downloads/results.sqlite # any well-defined database URL
```

:::note Relative vs absolute paths

If you define a _relative_ database URL, like `sql_con=sqlite:///./results.sqlite`, the results will be saved right where Hydra stores its individual run files. In other words, multiple `.sqlite` files are stored in the `./multirun` subfolders.

To prevent this, and store all results in 1 `.sqlite` file, use an **absolute** path, like above. But preferably, you are using a proper running database - see the recipes for more instructions on this.

:::

We are now ready to run an experiment. In a terminal, `cd` into the unzipped example directory and run the following:
```shell
python benchmark.py --multirun ranker='glob(*)' +callbacks.to_sql.url=$sql_con
```

And our experiment starts running üëèüèª!

<AsciinemaPlayer src="/fseval/cast/examples_quick_start.cast" rows={30} idleTimeLimit={3} preload={true} />
<br/>

Using `--multirun` combined with `ranker='glob(*)'` we instructed fseval to run experiments for all available rankers. The results are now stored in a SQLite database.

```shell
$ tree ~/Downloads
/Users/dunnkers/Downloads
‚îî‚îÄ‚îÄ results.sqlite

0 directories, 1 file
```

We can open the data using [DB Browser for SQLite](https://sqlitebrowser.org/). We can access the validation scores in the `validation_scores` table:

![validation data](/img/quick-start/validation_data.png)

In the example above, 
the graph plots the **feature subset size** (`n_features_to_select`) vs. **classification accuracy** (`score`).

For our two feature selectors, <span style={{backgroundColor: "#5050FF", color: "white"}}>ANOVA F value</span> vs. <span style={{backgroundColor: "#0075DD", color: "white"}}>Mutual Info</span>, we can now see which gets the highest classification accuracy with which feature subset.
Using fseval, we can easily compare many feature- selectors or rankers, and at a large scale üôèüèª.