
@inproceedings{zhao_searching_2007,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'07},
	title = {Searching for interacting features},
	url = {https://www.semanticscholar.org/paper/Searching-for-Interacting-Features-Zhao-Liu/d2debe138a9b67d838b11d622651383322934aee},
	abstract = {Feature interaction presents a challenge to feature selection for classification. A feature by itself may have little correlation with the target concept, but when it is combined with some other features, they can be strongly correlated with the target concept. Unintentional removal of these features can result in poor classification performance. Handling feature interaction can be computationally intractable. Recognizing the presence of feature interaction, we propose to efficiently handle feature interaction to achieve efficient feature selection and present extensive experimental results of evaluation.},
	urldate = {2020-10-21},
	booktitle = {Proceedings of the 20th international joint conference on {Artifical} intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Zhao, Zheng and Liu, Huan},
	month = jan,
	year = {2007},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {1156--1161},
	file = {Zhao and Liu - 2007 - Searching for interacting features.pdf:/Users/dunnkers/Zotero/storage/A4A22S6D/Zhao and Liu - 2007 - Searching for interacting features.pdf:application/pdf},
}

@inproceedings{kira_feature_1992,
	title = {The {Feature} {Selection} {Problem}: {Traditional} {Methods} and a {New} {Algorithm}},
	shorttitle = {The {Feature} {Selection} {Problem}},
	abstract = {For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Rellef which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as nonoptimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems.},
	booktitle = {{AAAI}},
	author = {Kira, K. and Rendell, L.},
	year = {1992},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	file = {Kira and Rendell - 1992 - The Feature Selection Problem Traditional Methods.pdf:/Users/dunnkers/Zotero/storage/YHIZJJ7P/Kira and Rendell - 1992 - The Feature Selection Problem Traditional Methods.pdf:application/pdf},
}

@article{bennasar_feature_2015,
	title = {Feature selection using {Joint} {Mutual} {Information} {Maximisation}},
	volume = {42},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417415004674},
	doi = {10.1016/j.eswa.2015.07.007},
	abstract = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the ‘maximum of the minimum’ criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6\% in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
	language = {en},
	number = {22},
	urldate = {2020-09-23},
	journal = {Expert Systems with Applications},
	author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
	month = dec,
	year = {2015},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {8520--8532},
	file = {Bennasar et al. - 2015 - Feature selection using Joint Mutual Information M.pdf:/Users/dunnkers/Zotero/storage/EUCS5EI2/Bennasar et al. - 2015 - Feature selection using Joint Mutual Information M.pdf:application/pdf},
}

@inproceedings{roffo_infinite_2015,
	address = {Santiago, Chile},
	title = {Infinite {Feature} {Selection}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410835/},
	doi = {10.1109/ICCV.2015.478},
	abstract = {Filter-based feature selection has become crucial in many classiﬁcation settings, especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper, we propose a feature selection method exploiting the convergence properties of power series of matrices, and introducing the concept of inﬁnite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions and letting these paths tend to an inﬁnite number permits the investigation of the importance (relevance and redundancy) of a feature when injected into an arbitrary set of cues. Ranking the importance individuates candidate features, which turn out to be effective from a classiﬁcation point of view, as proved by a thoroughly experimental section. The Inf-FS has been tested on thirteen diverse benchmarks, comparing against ﬁlters, embedded methods, and wrappers; in all the cases we achieve top performances, notably on the classiﬁcation tasks of PASCAL VOC 2007-2012.},
	language = {en},
	urldate = {2020-09-23},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Roffo, Giorgio and Melzi, Simone and Cristani, Marco},
	month = dec,
	year = {2015},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {4202--4210},
	file = {Roffo et al. - 2015 - Infinite Feature Selection.pdf:/Users/dunnkers/Zotero/storage/XS3N5QT8/Roffo et al. - 2015 - Infinite Feature Selection.pdf:application/pdf},
}

@inproceedings{almuallim_learning_1991,
	title = {Learning {With} {Many} {Irrelevant} {Features}},
	abstract = {In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires {\textbackslash}Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  +  p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...},
	booktitle = {In {Proceedings} of the {Ninth} {National} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Almuallim, Hussein and Dietterich, Thomas G.},
	year = {1991},
	keywords = {novel algorithm, read 20\%, read 40\%, read 60\%},
	pages = {547--552},
	file = {Citeseer - Snapshot:/Users/dunnkers/Zotero/storage/R6TQX3CL/download.html:text/html;Citeseer - Full Text PDF:/Users/dunnkers/Zotero/storage/T5KP4QH5/Almuallim and Dietterich - 1991 - Learning With Many Irrelevant Features.pdf:application/pdf;Almuallim and Dietterich - 1991 - Learning With Many Irrelevant Features..pdf:/Users/dunnkers/Zotero/storage/LVJ7TLVF/Almuallim and Dietterich - 1991 - Learning With Many Irrelevant Features..pdf:application/pdf},
}

@article{gu_generalized_2012,
	title = {Generalized {Fisher} {Score} for {Feature} {Selection}},
	url = {http://arxiv.org/abs/1202.3725},
	abstract = {Fisher score is one of the most widely used supervised feature selection methods. However, it selects each feature independently according to their scores under the Fisher criterion, which leads to a suboptimal subset of features. In this paper, we present a generalized Fisher score to jointly select features. It aims at finding an subset of features, which maximize the lower bound of traditional Fisher score. The resulting feature selection problem is a mixed integer programming, which can be reformulated as a quadratically constrained linear programming (QCLP). It is solved by cutting plane algorithm, in each iteration of which a multiple kernel learning problem is solved alternatively by multivariate ridge regression and projected gradient descent. Experiments on benchmark data sets indicate that the proposed method outperforms Fisher score as well as many other state-of-the-art feature selection methods.},
	urldate = {2021-04-20},
	journal = {arXiv:1202.3725 [cs, stat]},
	author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.3725},
	keywords = {novel algorithm, read 20\%, read 40\%},
	file = {arXiv Fulltext PDF:/Users/dunnkers/Zotero/storage/HL4GMHZT/Gu et al. - 2012 - Generalized Fisher Score for Feature Selection.pdf:application/pdf;arXiv.org Snapshot:/Users/dunnkers/Zotero/storage/4LFV2HIJ/1202.html:text/html},
}

@inproceedings{bradley_feature_1998,
	title = {Feature {Selection} via {Concave} {Minimization} and {Support} {Vector} {Machines}},
	abstract = {Computational comparison is made between two feature selection approaches for finding a separating plane that discriminates between two point sets in an n-dimensional feature space that utilizes as few of the n features (dimensions) as possible. In the concave minimization approach [19, 5] a separating plane is generated by minimizing a weighted sum of distances of misclassified points to two parallel planes that bound the sets and which determine the separating plane midway between them. Furthermore, the number of dimensions of the space used to determine the plane is minimized. In the support vector machine approach [27, 7, 1, 10, 24, 28], in addition to minimizing the weighted sum of distances of misclassified points to the bounding planes, we also maximize the distance between the two bounding planes that generate the separating plane. Computational results show that feature suppression is an indirect consequence of the support vector machine approach when an appropriate norm is us...},
	booktitle = {Machine {Learning} {Proceedings} of the {Fifteenth} {International} {Conference}({ICML} ’98},
	publisher = {Morgan Kaufmann},
	author = {Bradley, P. S. and Mangasarian, O. L.},
	year = {1998},
	keywords = {novel algorithm, read 20\%, to read},
	pages = {82--90},
	file = {Citeseer - Full Text PDF:/Users/dunnkers/Zotero/storage/XLLB86LY/Bradley and Mangasarian - 1998 - Feature Selection via Concave Minimization and Sup.pdf:application/pdf;Citeseer - Snapshot:/Users/dunnkers/Zotero/storage/EQFMCEJ9/download.html:text/html;Citeseer - Full Text PDF:/Users/dunnkers/Zotero/storage/9QBGDZIY/Bradley and Mangasarian - 1998 - Feature Selection via Concave Minimization and Sup.pdf:application/pdf;Citeseer - Snapshot:/Users/dunnkers/Zotero/storage/VQMF4T4J/download.html:text/html},
}

@article{wojtas_feature_2020,
	title = {Feature {Importance} {Ranking} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/2010.08973},
	abstract = {Feature importance ranking has become a powerful tool for explainable AI. However, its nature of combinatorial optimization poses a great challenge for deep learning. In this paper, we propose a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset of a fixed size and ranking the importance of those features in the optimal subset simultaneously. During learning, the operator is trained for a supervised learning task via optimal feature subset candidates generated by the selector that learns predicting the learning performance of the operator working on different optimal subset candidates. We develop an alternate learning algorithm that trains two nets jointly and incorporates a stochastic local search procedure into learning to address the combinatorial optimization challenge. In deployment, the selector generates an optimal feature subset and ranks feature importance, while the operator makes predictions based on the optimal subset for test data. A thorough evaluation on synthetic, benchmark and real data sets suggests that our approach outperforms several state-of-the-art feature importance ranking and supervised feature selection methods. (Our source code is available: https://github.com/maksym33/FeatureImportanceDL)},
	urldate = {2021-04-24},
	journal = {arXiv:2010.08973 [cs]},
	author = {Wojtas, Maksymilian and Chen, Ke},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08973},
	keywords = {to read},
	file = {arXiv Fulltext PDF:/Users/dunnkers/Zotero/storage/54LUDTAG/Wojtas and Chen - 2020 - Feature Importance Ranking for Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/dunnkers/Zotero/storage/2AXL9HE2/2010.html:text/html},
}

@article{peng_feature_2005,
	title = {Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	volume = {27},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2005.159},
	abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Peng, Hanchuan and Long, Fuhui and Ding, C.},
	month = aug,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {to read},
	pages = {1226--1238},
	file = {IEEE Xplore Abstract Record:/Users/dunnkers/Zotero/storage/7RBEJRK5/1453511.html:text/html;Peng et al. - 2005 - Feature selection based on mutual information crit.pdf:/Users/dunnkers/Zotero/storage/XSIRYM3G/Peng et al. - 2005 - Feature selection based on mutual information crit.pdf:application/pdf},
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	issn = {1532-4435},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	number = {null},
	journal = {The Journal of Machine Learning Research},
	author = {Guyon, Isabelle and Elisseeff, André},
	month = mar,
	year = {2003},
	pages = {1157--1182},
	file = {Full Text PDF:/Users/dunnkers/Zotero/storage/V3CTESSZ/Guyon and Elisseeff - 2003 - An introduction to variable and feature selection.pdf:application/pdf},
}

@incollection{bayer_sqlalchemy_2012,
	title = {{SQLAlchemy}},
	url = {http://aosabook.org/en/sqlalchemy.html},
	booktitle = {The {Architecture} of {Open} {Source} {Applications} {Volume} {II}: {Structure}, {Scale}, and a {Few} {More} {Fearless} {Hacks}},
	publisher = {aosabook.org},
	author = {Bayer, Michael},
	editor = {Brown, Amy and Wilson, Greg},
	year = {2012},
}

@misc{biewald_experiment_2020,
	title = {Experiment {Tracking} with {Weights} and {Biases}},
	url = {https://www.wandb.com/},
	author = {Biewald, Lukas},
	year = {2020},
}

@inproceedings{yoo_slurm_2003,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{SLURM}: {Simple} {Linux} {Utility} for {Resource} {Management}},
	isbn = {978-3-540-39727-4},
	shorttitle = {{SLURM}},
	doi = {10.1007/10968987_3},
	abstract = {A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.},
	language = {en},
	booktitle = {Job {Scheduling} {Strategies} for {Parallel} {Processing}},
	publisher = {Springer},
	author = {Yoo, Andy B. and Jette, Morris A. and Grondona, Mark},
	editor = {Feitelson, Dror and Rudolph, Larry and Schwiegelshohn, Uwe},
	year = {2003},
	pages = {44--60},
	file = {Submitted Version:/Users/dunnkers/Zotero/storage/H3MKAE5C/Yoo et al. - 2003 - SLURM Simple Linux Utility for Resource Managemen.pdf:application/pdf},
}

@misc{yadan_hydra_2019,
	title = {Hydra - {A} framework for elegantly configuring complex applications},
	url = {https://github.com/facebookresearch/hydra},
	author = {Yadan, Omry},
	year = {2019},
}

@article{vanschoren_openml_2013,
	title = {{OpenML}: {Networked} {Science} in {Machine} {Learning}},
	volume = {15},
	url = {http://doi.acm.org/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	number = {2},
	journal = {SIGKDD Explorations},
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	year = {2013},
	note = {Place: New York, NY, USA
Publisher: ACM},
	pages = {49--60},
}

@article{nogueira_stability_2018,
	title = {On the {Stability} of {Feature} {Selection} {Algorithms}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/17-514.html},
	abstract = {Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The âstabilityâ of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is `unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging---we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms.},
	number = {174},
	urldate = {2022-01-29},
	journal = {Journal of Machine Learning Research},
	author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
	year = {2018},
	pages = {1--54},
	file = {Full Text PDF:/Users/dunnkers/Zotero/storage/CXXAK82C/Nogueira et al. - 2018 - On the Stability of Feature Selection Algorithms.pdf:application/pdf},
}

@article{reis_featsel_2017,
	title = {featsel: A framework for benchmarking of feature selection algorithms and cost functions},
	journal = {SoftwareX},
	volume = {6},
	pages = {193-197},
	year = {2017},
	issn = {2352-7110},
	doi = {https://doi.org/10.1016/j.softx.2017.07.005},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711017300286},
	author = {Marcelo S. Reis and Gustavo Estrela and Carlos Eduardo Ferreira and Junior Barrera},
	keywords = {Feature selection, Benchmarking, Boolean lattice, Combinatorial optimization},
	abstract = {In this paper, we introduce featsel, a framework for benchmarking of feature selection algorithms and cost functions. This framework allows the user to deal with the search space as a Boolean lattice and has its core coded in C++ for computational efficiency purposes. Moreover, featsel includes Perl scripts to add new algorithms and/or cost functions, generate random instances, plot graphs and organize results into tables. Besides, this framework already comes with dozens of algorithms and cost functions for benchmarking experiments. We also provide illustrative examples, in which featsel outperforms the popular Weka workbench in feature selection procedures on data sets from the UCI Machine Learning Repository.}
}

@article{cilia2019experimental,
  title={An experimental comparison of feature-selection and classification methods for microarray datasets},
  author={Cilia, Nicole Dalia and De Stefano, Claudio and Fontanella, Francesco and Raimondo, Stefano and Scotto di Freca, Alessandra},
  journal={Information},
  volume={10},
  number={3},
  pages={109},
  year={2019},
  publisher={MDPI}
}
 
@article{sun2019comparison,
  title={Comparison of feature selection methods and machine learning classifiers for radiomics analysis in glioma grading},
  author={Sun, Pan and Wang, Defeng and Mok, Vincent Ct and Shi, Lin},
  journal={IEEE Access},
  volume={7},
  pages={102010--102020},
  year={2019},
  publisher={IEEE}
}

@article{cilia2019experimental,
  title={An experimental comparison of feature-selection and classification methods for microarray datasets},
  author={Cilia, Nicole Dalia and De Stefano, Claudio and Fontanella, Francesco and Raimondo, Stefano and Scotto di Freca, Alessandra},
  journal={Information},
  volume={10},
  number={3},
  pages={109},
  year={2019},
  publisher={MDPI}
}
 
@article{sun2019comparison,
  title={Comparison of feature selection methods and machine learning classifiers for radiomics analysis in glioma grading},
  author={Sun, Pan and Wang, Defeng and Mok, Vincent Ct and Shi, Lin},
  journal={IEEE Access},
  volume={7},
  pages={102010--102020},
  year={2019},
  publisher={IEEE}
}
 
 @article{tohka2016comparison,
  title={Comparison of feature selection techniques in machine learning for anatomical brain MRI in dementia},
  author={Tohka, Jussi and Moradi, Elaheh and Huttunen, Heikki},
  journal={Neuroinformatics},
  volume={14},
  number={3},
  pages={279--296},
  year={2016},
  publisher={Springer}
}
 
 
@article{ashok2016comparison,
  title={Comparison of Feature selection methods for diagnosis of cervical cancer using SVM classifier},
  author={Ashok, B and Aruna, P},
  journal={Int. J. Eng. Res. Appl},
  volume={6},
  pages={94--99},
  year={2016}
}
 
@article{liu2017multi,
  title={Multi-class sentiment classification: The experimental comparisons of feature selection and machine learning algorithms},
  author={Liu, Yang and Bi, Jian-Wu and Fan, Zhi-Ping},
  journal={Expert Systems with Applications},
  volume={80},
  pages={323--339},
  year={2017},
  publisher={Elsevier}
}
 
@article{kou2020evaluation,
  title={Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods},
  author={Kou, Gang and Yang, Pei and Peng, Yi and Xiao, Feng and Chen, Yang and Alsaadi, Fawaz E},
  journal={Applied Soft Computing},
  volume={86},
  pages={105836},
  year={2020},
  publisher={Elsevier}
}

@article{venkatesh2019review,
  title={A review of feature selection and its methods},
  author={Venkatesh, B and Anuradha, J},
  journal={Cybernetics and Information Technologies},
  volume={19},
  number={1},
  pages={3--26},
  year={2019}
}